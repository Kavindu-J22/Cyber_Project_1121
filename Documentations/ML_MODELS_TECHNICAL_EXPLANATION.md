# Machine Learning Models - Technical Explanation
## Zero Trust Telehealth Platform - Behavioral Biometric Authentication

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Voice Authentication Models](#1-voice-authentication-models)
3. [Keystroke Dynamics Models](#2-keystroke-dynamics-models)
4. [Mouse Movement Analysis Models](#3-mouse-movement-analysis-models)
5. [Model Comparison Summary](#model-comparison-summary)
6. [Why These Specific Models?](#why-these-specific-models)

---

## Overview

This document provides a comprehensive explanation of each machine learning component used in the Zero Trust Telehealth Platform. Each biometric modality uses specialized deep learning models chosen for their specific strengths in behavioral pattern recognition.

### Three-Layer ML Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BIOMETRIC MODALITIES                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Voice Authentication  â”‚  Keystroke Dynamics  â”‚  Mouse      â”‚
â”‚  ECAPA-TDNN + CNN     â”‚  Deep Neural Net     â”‚  Siamese    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Voice Authentication Models

### 1.1 Primary Model: ECAPA-TDNN

**Full Name:** Emphasized Channel Attention, Propagation and Aggregation in Time Delay Neural Network

#### Architecture Overview

```
Audio Input (16kHz WAV)
    â†“
[Mel-Filterbank Features] (80 mel-bins)
    â†“
[ECAPA-TDNN Encoder]
    â”œâ”€â”€ Conv1D Layers with varying dilations [1, 2, 3, 4, 1]
    â”œâ”€â”€ Channel Attention Mechanism (SE-Res2Net blocks)
    â”œâ”€â”€ Multi-scale feature aggregation
    â””â”€â”€ Temporal pooling (statistics pooling)
    â†“
[192-Dimensional Speaker Embedding]
    â†“
[Cosine Similarity Verification]
```

#### Model Specifications

| Parameter | Value | Purpose |
|-----------|-------|---------|
| **Input Size** | 80 mel-bins | Frequency representation of audio |
| **Channels** | [1024, 1024, 1024, 1024, 3072] | Multi-scale feature extraction |
| **Kernel Sizes** | [5, 3, 3, 3, 1] | Temporal context windows |
| **Dilations** | [1, 2, 3, 4, 1] | Capture long-range dependencies |
| **Attention Channels** | 128 | Channel-wise attention mechanism |
| **Embedding Dimension** | 192 | Final speaker representation |
| **Pre-training Dataset** | VoxCeleb1 & VoxCeleb2 | 7,000+ speakers, 1M+ utterances |

#### Why ECAPA-TDNN for Voice?

**1. State-of-the-Art Performance**
- Achieves < 3% Equal Error Rate (EER) on speaker verification
- Winner of VoxCeleb Speaker Recognition Challenge
- Outperforms traditional i-vectors and x-vectors

**2. Channel Attention Mechanism**
- **SE-Res2Net blocks** emphasize discriminative frequency channels
- Automatically learns which acoustic features are most important
- Robust to background noise and channel variations

**3. Multi-Scale Temporal Modeling**
- **Time Delay Neural Network (TDNN)** captures temporal patterns
- **Varying dilations** [1, 2, 3, 4, 1] capture both short and long-term dependencies
- Essential for speaker-specific prosody and rhythm

**4. Compact Embeddings**
- 192-dimensional embeddings are small (< 2KB encrypted)
- Fast cosine similarity computation (< 50ms)
- Suitable for real-time continuous authentication

**5. Transfer Learning**
- Pre-trained on VoxCeleb (7,000+ speakers)
- Generalizes well to new speakers with few-shot learning
- No need for extensive dataset collection

#### Training Strategy

```python
# Pre-trained model (Production-ready)
model = EncoderClassifier.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    savedir="models/pretrained/ecapa_tdnn"
)

# Loss Function: Additive Angular Margin (AAM-Softmax)
# - Enforces larger inter-speaker margins
# - Improves embedding discriminability
```

---

### 1.2 Anti-Spoofing Model: Lightweight CNN

#### Architecture Overview

```
Audio Input (Spectrogram)
    â†“
[Conv2D Layer 1] (32 filters, 3x3 kernel)
    â”œâ”€â”€ BatchNorm2D
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ MaxPool2D (2x2)
    â†“
[Conv2D Layer 2] (64 filters, 3x3 kernel)
    â”œâ”€â”€ BatchNorm2D
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ MaxPool2D (2x2)
    â†“
[Conv2D Layer 3] (128 filters, 3x3 kernel)
    â”œâ”€â”€ BatchNorm2D
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ AdaptiveAvgPool2D
    â†“
[Fully Connected Layers]
    â”œâ”€â”€ Linear(128 â†’ 64) + ReLU + Dropout(0.3)
    â””â”€â”€ Linear(64 â†’ 1) [Binary Classification]
    â†“
[Sigmoid Output] (Genuine vs. Spoofed)
```

#### Model Specifications

| Layer | Configuration | Purpose |
|-------|--------------|---------|
| **Conv2D-1** | 32 filters, 3x3, padding=1 | Low-level artifact detection |
| **Conv2D-2** | 64 filters, 3x3, padding=1 | Mid-level pattern recognition |
| **Conv2D-3** | 128 filters, 3x3, padding=1 | High-level spoofing signatures |
| **Dropout** | 0.3 | Prevent overfitting |
| **Output** | Binary (0=Genuine, 1=Spoofed) | Spoofing detection |

#### Why CNN for Anti-Spoofing?

**1. Spatial Pattern Recognition**
- **Convolutional layers** detect artifacts in spectrograms
- Identifies synthetic speech patterns (TTS artifacts)
- Detects replay attack characteristics (channel distortions)

**2. Hierarchical Feature Learning**
- **Layer 1:** Low-level artifacts (quantization noise, codec artifacts)
- **Layer 2:** Mid-level patterns (unnatural formant transitions)
- **Layer 3:** High-level signatures (voice cloning artifacts)

**3. Lightweight Architecture**
- Only 3 convolutional layers (< 5MB model size)
- Fast inference (< 100ms per audio segment)
- Suitable for real-time deployment

**4. Detection Capabilities**
- **Replay Attacks:** Detects channel mismatch and environmental noise inconsistencies
- **Synthetic Speech (TTS):** Identifies unnatural prosody and phase patterns
- **Voice Cloning:** Detects deep fake artifacts and spectral anomalies

#### Training Details

```python
# Loss Function: Binary Cross-Entropy
criterion = nn.BCEWithLogitsLoss()

# Optimizer: Adam with weight decay
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# Data Augmentation:
# - Time stretching
# - Pitch shifting
# - Background noise injection
```

---

## 2. Keystroke Dynamics Models

### 2.1 Primary Model: Deep Neural Network with Siamese Architecture

#### Architecture Overview

```
Keystroke Features (31-dimensional)
    â†“
[Input Layer] (31 features)
    â”œâ”€â”€ Hold Times (H.key): 11 features
    â”œâ”€â”€ DD Times (keydown-keydown): 10 features
    â””â”€â”€ UD Times (keyup-keydown): 10 features
    â†“
[Hidden Layer 1] (256 neurons)
    â”œâ”€â”€ Linear Transformation
    â”œâ”€â”€ Batch Normalization
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ Dropout (0.3)
    â†“
[Hidden Layer 2] (512 neurons)
    â”œâ”€â”€ Linear Transformation
    â”œâ”€â”€ Batch Normalization
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ Dropout (0.3)
    â†“
[Hidden Layer 3] (256 neurons)
    â”œâ”€â”€ Linear Transformation
    â”œâ”€â”€ Batch Normalization
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ Dropout (0.3)
    â†“
[Hidden Layer 4] (128 neurons)
    â”œâ”€â”€ Linear Transformation
    â”œâ”€â”€ Batch Normalization
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ Dropout (0.3)
    â†“
[Embedding Layer] (128-dimensional)
    â””â”€â”€ L2 Normalization
    â†“
[Cosine Similarity Verification]
```

#### Model Specifications

| Parameter | Value | Purpose |
|-----------|-------|---------|
| **Input Dimension** | 31 features | Timing features from keystrokes |
| **Hidden Layers** | [256, 512, 256, 128] | Progressive feature abstraction |
| **Embedding Dimension** | 128 | Behavioral signature |
| **Dropout Rate** | 0.3 | Regularization |
| **Activation** | ReLU | Non-linear transformations |
| **Normalization** | Batch Normalization | Stable training |
| **Output Normalization** | L2 Normalization | Unit sphere embeddings |

#### Feature Extraction

**1. Hold Times (H.key)** - 11 features
```
Time duration a key is pressed down
Example: H.period = 0.1491 seconds
Captures: Typing pressure, finger strength patterns
```

**2. Keydown-Keydown Times (DD.key1.key2)** - 10 features
```
Time between pressing two consecutive keys
Example: DD.period.t = 0.3979 seconds
Captures: Typing rhythm, inter-key timing
```

**3. Keyup-Keydown Times (UD.key1.key2)** - 10 features
```
Time from releasing one key to pressing the next
Example: UD.period.t = 0.2488 seconds
Captures: Flight time, finger transition patterns
```

#### Why Deep Neural Network for Keystroke Dynamics?

**1. Non-Linear Pattern Recognition**
- **Deep architecture** captures complex behavioral patterns
- Typing rhythm is highly non-linear and person-specific
- Traditional statistical methods (e.g., Gaussian models) fail to capture complexity

**2. Siamese Architecture Benefits**
- **Shared weights** learn similarity metrics directly
- Compares two typing samples in embedding space
- Robust to day-to-day variations in typing speed

**3. Progressive Feature Abstraction**
- **Layer 1 (256):** Low-level timing patterns
- **Layer 2 (512):** Mid-level rhythm combinations
- **Layer 3 (256):** High-level behavioral signatures
- **Layer 4 (128):** Compact discriminative embeddings

**4. Regularization Strategy**
- **Batch Normalization:** Prevents internal covariate shift
- **Dropout (0.3):** Prevents overfitting to specific typing samples
- **L2 Normalization:** Ensures embeddings lie on unit hypersphere

**5. Metric Learning**
- **Triplet Loss:** Learns to separate genuine from impostor samples
- **Contrastive Loss:** Pulls genuine pairs together, pushes impostors apart
- **Hard Negative Mining:** Focuses on difficult impostor samples

#### Training Strategy

```python
# Loss Function: Triplet Loss with Hard Mining
loss = max(0, ||anchor - positive||Â² - ||anchor - negative||Â² + margin)

# Triplet Mining Strategies:
# - Hard: Most difficult negatives (highest similarity to anchor)
# - Semi-hard: Negatives within margin
# - All: All valid triplets

# Optimizer: Adam
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Learning Rate Scheduler: ReduceLROnPlateau
scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
```

#### Performance Metrics

| Metric | Value | Meaning |
|--------|-------|---------|
| **EER (Equal Error Rate)** | < 5% | False Accept Rate = False Reject Rate |
| **Accuracy** | 96% | Correct genuine/impostor classification |
| **Verification Latency** | < 500ms | Real-time authentication |
| **Embedding Size** | 128 floats (512 bytes) | Compact storage |

---

### 2.2 Anomaly Detection: Isolation Forest

#### Why Isolation Forest?

**1. Unsupervised Anomaly Detection**
- Detects unusual typing patterns without labeled anomaly data
- Identifies bot/script behavior, fatigue, or user substitution

**2. Efficient for High-Dimensional Data**
- Works well with 31-dimensional keystroke features
- Fast training and inference (< 10ms)

**3. Anomaly Types Detected**
- **Typing Too Fast:** Mean timing < 50% of baseline
- **Typing Too Slow:** Mean timing > 200% of baseline
- **Irregular Rhythm:** Standard deviation > 200% of baseline
- **Unusual Patterns:** Z-score > 3.0 for individual features

```python
# Isolation Forest Configuration
model = IsolationForest(
    contamination=0.1,  # Expected anomaly rate
    n_estimators=100,
    max_samples='auto',
    random_state=42
)
```

---

## 3. Mouse Movement Analysis Models

### 3.1 Primary Model: Siamese Neural Network

#### Architecture Overview

```
Mouse Movement Features (50+ dimensional)
    â†“
[Input Layer] (50+ features)
    â”œâ”€â”€ Velocity features (speed, direction, statistics)
    â”œâ”€â”€ Acceleration features
    â”œâ”€â”€ Jerk features (smoothness)
    â”œâ”€â”€ Curvature features
    â”œâ”€â”€ Click dynamics
    â”œâ”€â”€ Trajectory features
    â””â”€â”€ Temporal features
    â†“
[Hidden Layer 1] (256 neurons)
    â”œâ”€â”€ Linear Transformation
    â”œâ”€â”€ Batch Normalization
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ Dropout (0.3)
    â†“
[Hidden Layer 2] (512 neurons)
    â”œâ”€â”€ Linear Transformation
    â”œâ”€â”€ Batch Normalization
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ Dropout (0.3)
    â†“
[Hidden Layer 3] (512 neurons)
    â”œâ”€â”€ Linear Transformation
    â”œâ”€â”€ Batch Normalization
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ Dropout (0.3)
    â†“
[Hidden Layer 4] (256 neurons)
    â”œâ”€â”€ Linear Transformation
    â”œâ”€â”€ Batch Normalization
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ Dropout (0.3)
    â†“
[Hidden Layer 5] (128 neurons)
    â”œâ”€â”€ Linear Transformation
    â”œâ”€â”€ Batch Normalization
    â”œâ”€â”€ ReLU Activation
    â””â”€â”€ Dropout (0.3)
    â†“
[Embedding Layer] (128-dimensional)
    â””â”€â”€ L2 Normalization
    â†“
[Cosine Similarity Verification]
```

#### Model Specifications

| Parameter | Value | Purpose |
|-----------|-------|---------|
| **Input Dimension** | 50+ features | Comprehensive movement characteristics |
| **Hidden Layers** | [256, 512, 512, 256, 128] | Deep feature extraction |
| **Embedding Dimension** | 128 | Behavioral signature |
| **Dropout Rate** | 0.3 | Regularization |
| **Activation** | ReLU | Non-linear transformations |
| **Normalization** | Batch Normalization | Stable training |
| **Architecture** | Siamese (shared weights) | Similarity learning |

#### Feature Categories (50+ Features)

**1. Velocity Features**
```
- Speed: Instantaneous velocity magnitude (pixels/second)
- Direction: Movement angle (radians)
- Statistics: Mean, std, median, min, max, Q25, Q75, skew, kurtosis
Purpose: Captures movement speed patterns unique to each user
```

**2. Acceleration Features**
```
- Magnitude: Rate of velocity change
- Statistics: Mean, std, median, quartiles
Purpose: Identifies how users accelerate/decelerate the mouse
```

**3. Jerk Features**
```
- Jerk: Rate of acceleration change (smoothness metric)
- Statistics: Mean, std, median
Purpose: Measures movement smoothness (jerky vs. smooth)
```

**4. Curvature Features**
```
- Curvature: Path curvature at each point
- Angular Variation: Change in movement direction
- Statistics: Mean, std, median
Purpose: Captures trajectory patterns (straight vs. curved paths)
```

**5. Click Dynamics**
```
- Click Count: Number of clicks in sequence
- Click Intervals: Time between clicks
- Hold Duration: Time mouse button held down
Purpose: Identifies clicking behavior patterns
```

**6. Trajectory Features**
```
- Path Efficiency: Ratio of direct distance to actual path length
- Straightness: How straight the movement path is
- Direction Changes: Number of direction reversals
Purpose: Measures path planning and execution efficiency
```

**7. Temporal Features**
```
- Time Differences: Intervals between movements
- Pause Detection: Periods of no movement
- Pause Duration: Length of pauses
- Burst Detection: Rapid movement sequences
Purpose: Captures timing patterns and hesitation behavior
```

#### Why Siamese Neural Network for Mouse Movements?

**1. Similarity Metric Learning**
- **Shared weights** ensure consistent feature extraction
- Learns to measure behavioral similarity directly
- More effective than hand-crafted distance metrics

**2. Robust to Variability**
- Mouse movements vary based on task (clicking vs. scrolling)
- Siamese architecture learns task-invariant behavioral signatures
- Embeddings capture underlying behavioral patterns, not specific tasks

**3. Deep Feature Hierarchy**
- **Layer 1 (256):** Basic movement primitives (speed, direction)
- **Layer 2 (512):** Movement combinations (acceleration patterns)
- **Layer 3 (512):** Complex trajectories (path planning)
- **Layer 4 (256):** Behavioral motifs (user-specific patterns)
- **Layer 5 (128):** Discriminative behavioral signature

**4. Continuous Authentication**
- Generates embeddings for sliding windows of movements
- Compares against enrolled template continuously
- Detects user substitution in real-time

**5. Triplet Loss Training**
- **Anchor:** User's genuine mouse movements
- **Positive:** Same user's different session
- **Negative:** Different user's movements
- **Goal:** Minimize distance(anchor, positive), maximize distance(anchor, negative)

#### Training Strategy

```python
# Loss Function: Triplet Loss with Hard Mining
loss = max(0, ||anchor - positive||Â² - ||anchor - negative||Â² + margin)

# Hard Triplet Mining:
# - Selects hardest negatives (most similar to anchor)
# - Focuses learning on difficult cases
# - Improves discrimination capability

# Optimizer: Adam
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Learning Rate Scheduler: ReduceLROnPlateau
scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

# Data Augmentation:
# - Random noise injection (Â±5 pixels)
# - Time warping (Â±10% speed variation)
# - Rotation (Â±15 degrees)
```

#### Performance Metrics

| Metric | Value | Meaning |
|--------|-------|---------|
| **EER (Equal Error Rate)** | < 8% | False Accept Rate = False Reject Rate |
| **Verification Threshold** | 0.70 | Confidence score for acceptance |
| **Latency** | < 300ms | Real-time verification |
| **Min Movements** | 50 events | Minimum data for verification |
| **Embedding Size** | 128 floats (512 bytes) | Compact storage |

---

### 3.2 Anomaly Detection: Isolation Forest

#### Why Isolation Forest for Mouse Movements?

**1. Detects Automated Behavior**
- **Bots/Scripts:** Constant velocity, perfect linear movements
- **Remote Desktop:** High jitter, unusual pause patterns
- **Impossible Speed:** Movements faster than human capability

**2. Anomaly Types Detected**

```python
# Constant Velocity Detection
if velocity_std < 1.0:
    anomaly = "robotic_movement"

# Linear Movement Detection
if trajectory_efficiency > 0.98:
    anomaly = "automated_script"

# Impossible Speed Detection
if max_velocity > 5000 pixels/second:
    anomaly = "impossible_speed"

# Remote Desktop Detection
if jerk_std > baseline_jerk_std * 3.0:
    anomaly = "rdp_artifacts"
```

**3. Configuration**

```python
# Isolation Forest Setup
model = IsolationForest(
    contamination=0.1,  # 10% expected anomaly rate
    n_estimators=100,
    max_samples='auto',
    random_state=42
)

# Anomaly Score Threshold
threshold = 0.7  # Scores > 0.7 flagged as anomalies
```

---

## Model Comparison Summary

### Architecture Comparison

| Component | Model Type | Architecture | Embedding Dim | Parameters |
|-----------|-----------|--------------|---------------|------------|
| **Voice** | ECAPA-TDNN | CNN + Attention | 192 | ~6.5M |
| **Voice Anti-Spoof** | Lightweight CNN | 3-layer CNN | N/A | ~50K |
| **Keystroke** | Deep Neural Net | 4-layer MLP | 128 | ~500K |
| **Mouse** | Siamese Network | 5-layer MLP | 128 | ~800K |

### Performance Comparison

| Component | EER | Latency | Storage | Accuracy |
|-----------|-----|---------|---------|----------|
| **Voice** | < 3% | < 800ms | 2KB | 97%+ |
| **Keystroke** | < 5% | < 500ms | 512B | 96% |
| **Mouse** | < 8% | < 300ms | 512B | 92%+ |

### Training Strategy Comparison

| Component | Loss Function | Training Data | Pre-trained |
|-----------|--------------|---------------|-------------|
| **Voice** | AAM-Softmax | VoxCeleb (7K speakers) | âœ… Yes |
| **Keystroke** | Triplet Loss | DSL Dataset (51 users) | âŒ No |
| **Mouse** | Triplet Loss | Balabit (10 users) | âŒ No |

---

## Why These Specific Models?

### 1. Voice: ECAPA-TDNN

**Chosen Because:**
- âœ… **State-of-the-art** speaker verification performance
- âœ… **Channel attention** handles noise and channel variations
- âœ… **Pre-trained** on massive dataset (no custom training needed)
- âœ… **Compact embeddings** (192-dim) for fast comparison
- âœ… **Proven in production** (used by major tech companies)

**Alternatives Considered:**
- âŒ **i-vectors:** Older technology, lower accuracy (EER ~10%)
- âŒ **x-vectors:** Good but less accurate than ECAPA-TDNN
- âŒ **WavLM:** Larger model, slower inference, similar accuracy

### 2. Voice Anti-Spoofing: Lightweight CNN

**Chosen Because:**
- âœ… **Spatial pattern recognition** in spectrograms
- âœ… **Lightweight** (< 5MB) for real-time deployment
- âœ… **Hierarchical features** detect multiple attack types
- âœ… **Fast inference** (< 100ms)

**Alternatives Considered:**
- âŒ **RNN/LSTM:** Slower, more complex, similar accuracy
- âŒ **Transformer:** Overkill for this task, much slower
- âŒ **Traditional ML (SVM):** Lower accuracy, requires manual features

### 3. Keystroke: Deep Neural Network

**Chosen Because:**
- âœ… **Non-linear pattern recognition** for complex typing rhythms
- âœ… **Siamese architecture** learns similarity metrics directly
- âœ… **Compact embeddings** (128-dim) for efficient storage
- âœ… **Triplet loss** with hard mining improves discrimination
- âœ… **Batch normalization** ensures stable training

**Alternatives Considered:**
- âŒ **Random Forest:** Cannot capture temporal dependencies, lower accuracy
- âŒ **SVM:** Requires manual feature engineering, less flexible
- âŒ **RNN/LSTM:** Overkill for fixed-length sequences, slower
- âŒ **1D CNN:** Less effective for timing features than fully connected layers

**Why NOT Random Forest?**
- Random Forest is an ensemble of decision trees
- Works well for tabular data with independent features
- **Problem:** Keystroke timing features are highly correlated
- **Problem:** Cannot learn complex non-linear relationships
- **Problem:** No embedding space for similarity comparison
- **Result:** Lower accuracy (~85% vs. 96% with DNN)

### 4. Mouse: Siamese Neural Network

**Chosen Because:**
- âœ… **Similarity metric learning** for behavioral comparison
- âœ… **Deep architecture** (5 layers) captures complex movement patterns
- âœ… **Robust to task variability** (clicking, scrolling, dragging)
- âœ… **Triplet loss** with hard mining focuses on difficult cases
- âœ… **L2 normalization** ensures consistent embedding magnitudes

**Alternatives Considered:**
- âŒ **Random Forest:** Cannot capture sequential movement patterns
- âŒ **SVM:** Requires manual feature engineering, less accurate
- âŒ **RNN/LSTM:** Slower, more complex, similar accuracy for this task
- âŒ **1D CNN:** Less effective for aggregated statistical features

**Why NOT Random Forest?**
- Random Forest works on independent samples
- **Problem:** Mouse movements are sequential and context-dependent
- **Problem:** Cannot learn behavioral embeddings
- **Problem:** Requires manual threshold tuning for each user
- **Result:** Lower accuracy (~80% vs. 92% with Siamese Network)

---

## Technical Deep Dive: Why Deep Learning Over Traditional ML?

### Traditional ML Limitations

#### 1. Random Forest for Behavioral Biometrics

**How Random Forest Works:**
```
1. Build multiple decision trees
2. Each tree splits on feature thresholds
3. Majority vote for classification
```

**Why It Fails for Behavioral Biometrics:**

âŒ **No Temporal Modeling**
- Cannot capture sequential patterns (typing rhythm, mouse trajectories)
- Treats each feature independently
- Misses correlations between timing features

âŒ **No Embedding Space**
- Cannot create compact behavioral signatures
- Requires storing all training samples
- Slow comparison (must traverse trees)

âŒ **Poor Generalization**
- Overfits to specific typing/mouse patterns
- Cannot adapt to day-to-day variations
- Requires retraining for each new user

âŒ **No Similarity Learning**
- Cannot learn what makes two samples "similar"
- Relies on hard decision boundaries
- Poor performance on continuous authentication

**Example: Keystroke Dynamics with Random Forest**
```python
# Random Forest approach (Traditional ML)
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)  # X: timing features, y: user labels

# Problem 1: Requires labeled data for ALL users
# Problem 2: Cannot verify new users without retraining
# Problem 3: No behavioral embedding for comparison
# Result: ~85% accuracy, slow verification
```

**Example: Keystroke Dynamics with Deep Learning**
```python
# Deep Learning approach (Siamese Network)
model = KeystrokeEmbeddingModel(input_dim=31, embedding_dim=128)
embedding = model(keystroke_features)  # 128-dim behavioral signature

# Advantage 1: Works with few samples (few-shot learning)
# Advantage 2: Can verify new users immediately
# Advantage 3: Fast cosine similarity comparison
# Result: 96% accuracy, real-time verification
```

#### 2. SVM (Support Vector Machine)

**Why SVM Fails:**
- âŒ Requires manual kernel selection (RBF, polynomial, linear)
- âŒ Slow training on large datasets (O(nÂ²) to O(nÂ³))
- âŒ No automatic feature learning
- âŒ Poor scalability to high-dimensional embeddings

#### 3. Gaussian Mixture Models (GMM)

**Why GMM Fails:**
- âŒ Assumes Gaussian distribution (behavioral data is non-Gaussian)
- âŒ Requires manual selection of number of components
- âŒ Cannot capture complex non-linear patterns
- âŒ Poor performance on high-dimensional data

---

## Deep Learning Advantages for Behavioral Biometrics

### 1. Automatic Feature Learning

**Traditional ML:**
```python
# Manual feature engineering required
features = [
    mean(hold_times),
    std(hold_times),
    median(hold_times),
    # ... 50+ hand-crafted features
]
```

**Deep Learning:**
```python
# Automatic feature learning
embedding = model(raw_features)  # Model learns optimal features
```

### 2. Embedding Space for Similarity

**Traditional ML:**
```python
# No embedding space
prediction = rf.predict(sample)  # Binary: genuine or impostor
# Cannot measure "how similar" two samples are
```

**Deep Learning:**
```python
# Embedding space enables similarity measurement
embedding1 = model(sample1)  # 128-dim vector
embedding2 = model(sample2)  # 128-dim vector
similarity = cosine_similarity(embedding1, embedding2)  # 0.0 to 1.0
# Continuous confidence score, not just binary decision
```

### 3. Few-Shot Learning

**Traditional ML:**
```python
# Requires many samples per user
rf.fit(X_train, y_train)  # Needs 100+ samples per user
```

**Deep Learning:**
```python
# Works with few samples (few-shot learning)
template = model(enrollment_samples)  # Only 3-5 samples needed
# Model pre-trained on general behavioral patterns
```

### 4. Transfer Learning

**Traditional ML:**
```python
# No transfer learning
# Must train from scratch for each new dataset
```

**Deep Learning:**
```python
# Transfer learning from pre-trained models
model = EncoderClassifier.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb"  # Pre-trained on 7K speakers
)
# Fine-tune on your specific users (optional)
```

### 5. Continuous Authentication

**Traditional ML:**
```python
# Requires full re-verification each time
prediction = rf.predict(new_sample)  # Binary decision
```

**Deep Learning:**
```python
# Continuous confidence scoring
for window in sliding_windows:
    embedding = model(window)
    confidence = cosine_similarity(embedding, template)
    if confidence < threshold:
        trigger_alert()
# Real-time monitoring with confidence scores
```

---

## Loss Functions Explained

### 1. Triplet Loss (Keystroke & Mouse)

**Concept:**
```
Given:
- Anchor: User A's sample
- Positive: User A's different sample
- Negative: User B's sample

Goal:
- Make distance(anchor, positive) small
- Make distance(anchor, negative) large
```

**Mathematical Formula:**
```
L = max(0, ||anchor - positive||Â² - ||anchor - negative||Â² + margin)

Where:
- ||Â·||Â² is Euclidean distance squared
- margin is minimum separation (typically 0.2 to 1.0)
```

**Why Triplet Loss?**
- âœ… Learns relative similarity (not absolute classification)
- âœ… Creates discriminative embedding space
- âœ… Works with few samples per user
- âœ… Enables continuous authentication

**Hard Triplet Mining:**
```python
# Select hardest negatives (most similar to anchor)
negatives = find_samples_where(
    similarity(anchor, negative) > similarity(anchor, positive)
)
# Focus learning on difficult cases
```

### 2. Contrastive Loss (Alternative)

**Concept:**
```
Given:
- Sample 1
- Sample 2
- Label: 1 if same user, 0 if different user

Goal:
- Pull same-user pairs together
- Push different-user pairs apart
```

**Mathematical Formula:**
```
L = (1 - label) * ||embedding1 - embedding2||Â² +
    label * max(0, margin - ||embedding1 - embedding2||)Â²
```

### 3. AAM-Softmax (Voice)

**Concept:**
```
Additive Angular Margin Softmax
- Adds angular margin to classification
- Enforces larger inter-class separation
```

**Why for Voice?**
- âœ… State-of-the-art for speaker recognition
- âœ… Creates well-separated speaker embeddings
- âœ… Robust to intra-speaker variations

---

## Practical Implementation Details

### Voice Authentication Pipeline

```python
# 1. Audio Preprocessing
audio = load_audio(file_path, sample_rate=16000)
audio = apply_vad(audio)  # Voice Activity Detection
audio = reduce_noise(audio)  # Noise reduction
segments = segment_audio(audio, window=2.5, overlap=0.5)

# 2. Feature Extraction
mel_features = compute_mel_filterbank(audio, n_mels=80)

# 3. Embedding Extraction
embedding = ecapa_tdnn_model.encode_batch(mel_features)  # 192-dim

# 4. Anti-Spoofing Check
spectrogram = compute_spectrogram(audio)
is_genuine = anti_spoofing_cnn(spectrogram)  # Binary: genuine/spoofed

# 5. Verification
if is_genuine:
    similarity = cosine_similarity(embedding, enrolled_template)
    verified = similarity >= threshold  # threshold = 0.65
```

### Keystroke Dynamics Pipeline

```python
# 1. Capture Keystroke Events
events = capture_keystrokes()  # Frontend captures key events

# 2. Feature Extraction
features = extract_timing_features(events)
# - Hold times: [0.149, 0.152, ...]
# - DD times: [0.398, 0.401, ...]
# - UD times: [0.249, 0.251, ...]

# 3. Normalization
features = scaler.transform(features)  # Z-score normalization

# 4. Embedding Generation
embedding = keystroke_model(features)  # 128-dim

# 5. Verification
similarity = cosine_similarity(embedding, enrolled_template)
verified = similarity >= threshold  # threshold = 0.75

# 6. Anomaly Detection
anomaly_score = isolation_forest.score_samples(features)
is_anomaly = anomaly_score < -0.5
```

### Mouse Movement Pipeline

```python
# 1. Capture Mouse Events
events = capture_mouse_movements()  # Frontend captures mouse events

# 2. Feature Extraction (50+ features)
features = extract_movement_features(events)
# - Velocity: mean, std, median, quartiles
# - Acceleration: mean, std
# - Jerk: mean, std (smoothness)
# - Curvature: mean, std
# - Click dynamics: count, intervals
# - Trajectory: efficiency, straightness
# - Temporal: pauses, bursts

# 3. Normalization
features = scaler.transform(features)

# 4. Embedding Generation
embedding = mouse_model(features)  # 128-dim

# 5. Verification
similarity = cosine_similarity(embedding, enrolled_template)
verified = similarity >= threshold  # threshold = 0.70

# 6. Anomaly Detection
is_bot = detect_automated_behavior(features)
is_rdp = detect_remote_desktop(features)
```

---

## Conclusion

This Zero Trust Telehealth Platform uses a sophisticated multi-modal biometric authentication system powered by state-of-the-art deep learning models:

1. **Voice Authentication (ECAPA-TDNN + CNN)**
   - Pre-trained on 7,000+ speakers
   - 97%+ accuracy, < 3% EER
   - Real-time anti-spoofing detection

2. **Keystroke Dynamics (Deep Neural Network)**
   - 96% accuracy, < 5% EER
   - Continuous authentication every 30 seconds
   - Anomaly detection for bot/script detection

3. **Mouse Movement Analysis (Siamese Network)**
   - 92%+ accuracy, < 8% EER
   - 50+ behavioral features
   - Detects automated behavior and user substitution

**Why Deep Learning Over Traditional ML?**
- âœ… Automatic feature learning (no manual engineering)
- âœ… Embedding spaces for similarity measurement
- âœ… Few-shot learning (works with minimal enrollment data)
- âœ… Transfer learning (leverage pre-trained models)
- âœ… Continuous authentication with confidence scores
- âœ… Superior accuracy (96% vs. 85% with Random Forest)

**Key Takeaway:** Deep learning models are essential for behavioral biometrics because they can learn complex, non-linear patterns that traditional ML methods cannot capture. The combination of Siamese architectures, triplet loss, and embedding spaces enables robust, real-time continuous authentication for zero-trust security.

---

## References

### Academic Papers

1. **ECAPA-TDNN:** Desplanques et al., "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification" (2020)
2. **Triplet Loss:** Schroff et al., "FaceNet: A Unified Embedding for Face Recognition and Clustering" (2015)
3. **Keystroke Dynamics:** Killourhy & Maxion, "Comparing Anomaly-Detection Algorithms for Keystroke Dynamics" (2009)
4. **Mouse Dynamics:** Shen et al., "User Authentication through Mouse Dynamics" (2013)

### Datasets

1. **VoxCeleb:** 7,000+ speakers, 1M+ utterances
2. **DSL-StrongPasswordData:** 51 users, keystroke timing data
3. **Balabit Mouse Dynamics Challenge:** 10 users, mouse movement data

### Frameworks & Libraries

1. **PyTorch:** Deep learning framework
2. **SpeechBrain:** Speaker recognition toolkit
3. **Scikit-learn:** Anomaly detection (Isolation Forest)
4. **FastAPI:** ML model serving

---

**Document Version:** 1.0
**Last Updated:** December 9, 2025
**Author:** Cyber Security Team
**Project:** Zero Trust Telehealth Platform


